\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]    {fontenc }% Allow accented output charachters
\usepackage[utf8]  {inputenc}% Allow accented input charachters 
\usepackage        {lmodern }% Modern output font     
\usepackage{geometry}
\usepackage[backend=bibtex, sorting=none]{biblatex}
\bibliography{references}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}

\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=90)
@

<<download,echo=FALSE,eval=TRUE,cache=TRUE, warning=FALSE>>=
dir.create("data")
download.file( "https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda",
               "./data/samsungData.rda", method="curl")
load("./data/samsungData.rda")
@


<<drop_invalid,echo=FALSE,eval=TRUE,cache=FALSE, warning=FALSE>>=
# clean invalid charachters from the data set column names
samsungData <- data.frame(samsungData)
# make activity a factor variable
samsungData$activity <- as.factor(samsungData$activity)
@


<<duplicate_columns,echo=FALSE,eval=TRUE,cache=FALSE, warning=FALSE>>=
# drop duplicated columns
noDupCol <- samsungData[!duplicated(lapply(samsungData, summary))]
@



<<miss_rate_function,echo=FALSE,eval=TRUE,cache=FALSE, warning=FALSE>>=
miss.rate <- function(decisionTree, dataSet) {
  class.pred <- table(predict(decisionTree, dataSet, type="class"), dataSet$activity)
  miss <- 1-sum(diag(class.pred))/sum(class.pred)
  r <- list(t=class.pred, miss=miss)
  return (r)
}
@

<<data_sets,echo=FALSE,eval=TRUE,cache=FALSE, warning=FALSE>>=
trainingSet    <- noDupCol[ noDupCol$subject %in% c(1,3,5,6), 
                      !(names(noDupCol) %in% c('subject'))]
evaluationSet  <- noDupCol[ noDupCol$subject %in% c(27,28,29,30),
                      !(names(noDupCol) %in% c('subject')) ]
validationSet  <- noDupCol[ noDupCol$subject %in% c(27,28,29,30, 1, 3, 5, 6),
                           !(names(noDupCol) %in% c('subject')) ]
@


<<tree_create,echo=FALSE,eval=TRUE,cache=FALSE, warning=TRUE>>=
library(tree)
decisionTree <- tree(formula=activity ~ . , data=trainingSet)
@

<<prunning,echo=FALSE,eval=TRUE,cache=FALSE, warning=TRUE>>=
library(ggplot2)
prunning <- function(decisionTree, dataSet, n) {
  # iterate over the number of decisions and get miss classification
  miss=vector()
  for ( i in 2:n ) {
    pruned <- prune.tree(decisionTree, best=i)
    miss = c(miss, miss.rate(pruned, dataSet)$miss)
  }
  # create a plot of the miss classification values
  plot = ggplot(data=data.frame(a=as.factor(2:n), m=miss),aes(a, m)) +
         geom_bar(stat='identity') +
         xlab("Decision count in tree") + 
         ylab("Miss classification rate")+
         theme(text = element_text(size=10),
               axis.title.x = element_text(colour = "#999999"), 
               axis.title.y = element_text(colour = "#999999"))
  return (list(miss=miss, plot=plot))
}
@

<<prunning_train,echo=FALSE,eval=TRUE,cache=FALSE, warning=TRUE, fig.cap="Miss classification for training set. Increasing the number of decisions decreases the miss classfication as we fit better and beter the training set.", fig.height=2.5>>=
trainPrun <- prunning(decisionTree, trainingSet, 12)
trainPrun$plot + ggtitle("Training set miss classification ratio")
@

<<prunning_eval,echo=FALSE,eval=TRUE,cache=FALSE, warning=TRUE, fig.cap="Miss classification for evaluation set. There is little difference between 6 and 12 rules, supporting the overtraining effect of the tree.", fig.height=2.5>>=x`
evalPrun <- prunning(decisionTree, evaluationSet, 12)
evalPrun$plot + ggtitle("Evaluation set miss classification rates")
@

<<finalSelect,echo=FALSE,eval=TRUE,cache=FALSE, warning=TRUE, fig.cap="The optimal decision tree with 10 decisions to predict activity from mobile sensor information", fig.height=6>>=
choosenDecisionTree <- prune.tree(decisionTree, best=10)
library(ggdendro)
fitChoosen <- dendro_data(choosenDecisionTree, type="proportional")
ggplot() +
  geom_segment(data=fitChoosen$segments, aes(x=x, y=y, xend=xend, yend=yend)) +
  geom_text(data=fitChoosen$labels,aes(x=x, y=y, label=label), size=2.5, 
            vjust=-0.5, color = 'blue') +
  geom_text(data=fitChoosen$leaf_labels, aes(x=x, y=y, label=label), 
            color='red', size=3, vjust=1) +
  ggtitle("A ten rule decision tree for predicting activity from mobile phone sensor data")+
  theme_dendro()
@


\end{document}